{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018887fd-e9e8-495b-872f-fefbd9cd6cb5",
   "metadata": {},
   "source": [
    "To generate synthetic VisiumHD data from Xenium, please read and run all the cells below. Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c610b-e1b5-43e6-a35d-3548588cb652",
   "metadata": {},
   "source": [
    "## Download Xenium output from 10X website\n",
    "Paste the URL for the binned_outputs.tar.gz for the sample you want to analyze.\n",
    "\n",
    "1. Go to Xenium public datasets page:https://www.10xgenomics.com/datasets?query=&page=1&configure%5BhitsPerPage%5D=50&configure%5BmaxValuesPerFacet%5D=1000&refinementList%5Bproduct.name%5D%5B0%5D=In%20Situ%20Gene%20Expression&refinementList%5Bspecies%5D%5B0%5D=Human&refinementList%5BdiseaseStates%5D%5B0%5D=colorectal%20cancer\n",
    "\n",
    "2. Select sample to analyze scrolling down to downloads section, click \"Batch download\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f721b2b-4314-4528-9c01-185726147728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "xenium_outputs_url = \"https://cf.10xgenomics.com/samples/xenium/2.0.0/Xenium_V1_Human_Colorectal_Cancer_Addon_FFPE/Xenium_V1_Human_Colorectal_Cancer_Addon_FFPE_outs.zip\"\n",
    "# Step 1: Download the raw Xenium output\n",
    "!curl -O {xenium_outputs_url}\n",
    "\n",
    "# Extract the ZIP file\n",
    "zip_file_path = xenium_outputs_url.split(\"/\")[-1]\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"extracted_files\")\n",
    "\n",
    "print(\"Extraction completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fcd48a-2f55-43b4-befd-8d646ea634cf",
   "metadata": {},
   "source": [
    "### Install prerequisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453e3e3-a55c-47fb-ab83-2c3743833b89",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install scipy\n",
    "!pip install shapely\n",
    "!pip install tifffile\n",
    "!pip install plotly\n",
    "!pip install tensorflow-gpu==2.10.0\n",
    "!pip install stardist\n",
    "!pip install geopandas\n",
    "!pip install scanpy\n",
    "!pip install fastparquet\n",
    "!pip install opencv-python\n",
    "!pip install geojson\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79fb2c-0fd9-4bd4-8be9-4d1bd04d8733",
   "metadata": {},
   "source": [
    "### Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4dc02-2b8d-4e00-9cbd-8a4d151ca5af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd # Geopandas for storing Shapely objects\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import anndata\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import re\n",
    "import shapely\n",
    "from shapely.geometry import Polygon, Point # Representing bins and cells as Shapely Polygons and Point objects\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8d90a-65dd-4e93-b4e2-4a257d6e1dc7",
   "metadata": {},
   "source": [
    "### Load Cell & Transcripts Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb54b91-6757-467c-81d3-7a4f6916fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transcript data\n",
    "transcripts_path = \"extracted_files/transcripts.csv.gz\"\n",
    "with gzip.open(transcripts_path, 'rt') as f:\n",
    "    transcripts_df = pd.read_csv(f)\n",
    "\n",
    "# Load cell info\n",
    "cells_path = \"extracted_files/cells.csv.gz\"\n",
    "with gzip.open(cells_path, 'rt') as f:\n",
    "    cells_df = pd.read_csv(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac1dea-7855-4af4-8989-c2b63deed2f1",
   "metadata": {},
   "source": [
    "### Load Cell Boundary Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2bdf0-8871-4bb0-a38e-3f9c31c7b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "zarr_file = zarr.open('extracted_files/cells.zarr.zip', mode='r')\n",
    "print(zarr_file.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092c013-dd0d-47f5-a6cc-3491f1f62dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = zarr_file['polygon_sets/0/vertices'][:]\n",
    "# 1 is whole cell, 0 is nucleus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5ff74-6269-42b4-9a9e-604f520a7528",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create folders to store synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6839176-4a75-4f1e-b4f7-13899b946963",
   "metadata": {},
   "source": [
    "For both the `seqfish_dir` and `enact_data_dir`, change `\"/home/oneai/\"` to the directory that stores this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec69f53-4a93-491a-b6f0-652b27ffaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenium_dir = \"/home/oneai/oneai-dda-spatialtr-visiumhd_analysis/synthetic_data/xenium\" # Update it to the directory where you want to save the synthetic data\n",
    "enact_data_dir = \"/home/oneai/oneai-dda-spatialtr-visiumhd_analysis/cache/xenium_nuclei/chunks\" # Directory that saves all the input and results of the enact pipeline, \n",
    "# should end with \"oneai-dda-spatialtr-visiumhd_analysis/cache/seqfish/chunks\"\n",
    "\n",
    "transcripts_df_chunks_dir = os.path.join(xenium_dir, \"transcripts_patches\") # Directory to store the files that contain the transcripts info for each chunk\n",
    "output_dir = os.path.join(enact_data_dir, \"bins_gdf\") # Directory to store the results of gene-to-bin assignment for each chunk\n",
    "cells_df_chunks_dir =  os.path.join(enact_data_dir,\"cells_gdf\") \n",
    "ground_truth_dir =  os.path.join(xenium_dir, \"ground_truth_nuclei\")\n",
    "\n",
    "# Making relevant directories\n",
    "os.makedirs(xenium_dir, exist_ok=True)\n",
    "os.makedirs(enact_data_dir, exist_ok=True)\n",
    "os.makedirs(transcripts_df_chunks_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(cells_df_chunks_dir, exist_ok=True)\n",
    "os.makedirs(ground_truth_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe70a1-ed23-4cb6-a7b6-d35e4c01f895",
   "metadata": {},
   "source": [
    "### Generate Synthetic VesiumHD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd8461-7bcc-4101-b26b-765daf975916",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Break transcripts df to patches (based on location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b4ce0-30d1-4c23-9b2d-0622db0a4f8c",
   "metadata": {},
   "source": [
    "Break transcripts df to patches of size 1000um x 1000um (larger patch size may result in memory issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb886a-5893-40ba-b187-650d6cfb4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch size: 1000 um x 1000 um\n",
    "\n",
    "patch_size = 1000\n",
    "\n",
    "# patch indices\n",
    "transcripts_df['x_patch'] = (transcripts_df['x_location'] // patch_size).astype(int)\n",
    "transcripts_df['y_patch'] = (transcripts_df['y_location'] // patch_size).astype(int)\n",
    "transcripts_df[\"patch_id\"] = transcripts_df[\"x_patch\"].astype(str) + \"_\" + transcripts_df[\"y_patch\"].astype(str)\n",
    "\n",
    "# Create a df for each patch\n",
    "grouped = transcripts_df.groupby(['x_patch', 'y_patch'])\n",
    "for (x_patch, y_patch), group in grouped:\n",
    "    # Calculate the start and end locations for each patch\n",
    "    # x_start = x_patch * patch_size\n",
    "    # x_end = (x_patch + 1) * patch_size\n",
    "    # y_start = y_patch * patch_size\n",
    "    # y_end = (y_patch + 1) * patch_size\n",
    "    \n",
    "    filename = f\"patch_{x_patch}_{y_patch}.csv\"\n",
    "    output_loc = os.path.join(transcripts_df_patch_dir , filename)\n",
    "    group.to_csv(output_loc)\n",
    "\n",
    "    print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbc9ec-675b-4b25-8448-334ed317798a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate synthetic vesiumHD for each patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebc8fd-88e9-4b14-8470-a474085dee64",
   "metadata": {},
   "source": [
    "Each patch is broken into bins of size 2um x 2um. The synthetic data contains transcript counts orgnized by bin_id. Each row contains transcript counts for a unique bin. Bins with no transcript counts is not included. \n",
    "\n",
    "In addition to all the gene features, there are two additional columns represent the row number and column number of the bin, and a column contains the Shapely polygon item that represents the bin. The first column is the bin_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19155a0-5646-49bd-915c-94737e251bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_VesiumHD_data(transcripts_df, bin_size=2, whole_cell=True, QScore20=True):\n",
    "    filtered_df = transcripts_df.copy()\n",
    "    # only count transcripts in the nucleus\n",
    "    if not whole_cell:\n",
    "        filtered_df = transcripts_df[transcripts_df['overlaps_nucleus'] == 1].copy()\n",
    "    \n",
    "    only count transcripts with QScore >= 20\n",
    "    if QScore20:\n",
    "        filtered_df = filtered_df[filtered_df['qv'] >= 20].copy()\n",
    " \n",
    "    # assigne bin to each transcript\n",
    "    filtered_df.loc[:, 'row'] =np.ceil(filtered_df['y_location'] / bin_size).astype(int)\n",
    "    filtered_df.loc[:, 'column'] = np.ceil(filtered_df['x_location'] / bin_size).astype(int)\n",
    "    filtered_df.loc[:, 'assigned_bin_id'] = filtered_df.apply(\n",
    "        lambda row: f\"{bin_size}um_\" + str(row['row']).zfill(5) +\"_\"+ str(row['column']).zfill(5),\n",
    "        axis=1)\n",
    "    \n",
    "    bin_coordinates = filtered_df[['assigned_bin_id', 'row', 'column']].drop_duplicates().set_index('assigned_bin_id')\n",
    "    bin_gene_matrix = filtered_df.groupby(['assigned_bin_id', 'feature_name']).size().unstack(fill_value=0)\n",
    "    bin_gene_matrix_with_coords = bin_gene_matrix.merge(bin_coordinates, left_index=True, right_index=True)\n",
    "    \n",
    "    return bin_gene_matrix_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd804c49-dc85-4fa9-85d4-a621cf0598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract row and column number from the bin_id\n",
    "def extract_numbers(entry):\n",
    "    match = re.search(r'_(\\d{5})_(\\d{5})', entry)\n",
    "    if match:\n",
    "        number1 = int(match.group(1).lstrip('0'))  \n",
    "        number2 = int(match.group(2).lstrip('0'))  \n",
    "        return number2*2-1, number1*2-1\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d45c22-2776-4b80-a29b-37d07f6b06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_bin_polys(bins_df, x_col, y_col, bin_size):\n",
    "        \"\"\"Represents the bins as Shapely polygons\n",
    "\n",
    "        Args:\n",
    "            bins_df (pd.DataFrame): bins dataframe\n",
    "            x_col (str): column with the bin centre x-coordinate\n",
    "            y_col (str): column with the bin centre y-coordinate\n",
    "            bin_size (int): bin size in pixels\n",
    "\n",
    "        Returns:\n",
    "            list: list of Shapely polygons\n",
    "        \"\"\"\n",
    "        geometry = []\n",
    "        # Generates Shapely polygons to represent each bin\n",
    "\n",
    "        if True:\n",
    "            half_bin_size = bin_size / 2\n",
    "            bbox_coords = pd.DataFrame(\n",
    "                {\n",
    "                    \"min_x\": bins_df[x_col] - half_bin_size,\n",
    "                    \"min_y\": bins_df[y_col] - half_bin_size,\n",
    "                    \"max_x\": bins_df[x_col] + half_bin_size,\n",
    "                    \"max_y\": bins_df[y_col] + half_bin_size,\n",
    "                }\n",
    "            )\n",
    "            geometry = [\n",
    "                shapely.geometry.box(min_x, min_y, max_x, max_y)\n",
    "                for min_x, min_y, max_x, max_y in tqdm(\n",
    "                    zip(\n",
    "                        bbox_coords[\"min_x\"],\n",
    "                        bbox_coords[\"min_y\"],\n",
    "                        bbox_coords[\"max_x\"],\n",
    "                        bbox_coords[\"max_y\"],\n",
    "                    ),\n",
    "                    total=len(bins_df),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c4071-ff50-4ec1-bd0d-37c8ddecaa54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through all the transcripra_df chunks and generate gene-to-bin assignments \n",
    "patch_size = 1000\n",
    "bin_size = 2\n",
    "transcripts_df_chunks = os.listdir(transcripts_df_patch_dir)\n",
    "for chunk_fname in transcripts_df_chunks:\n",
    "    output_loc = os.path.join(output_dir, chunk_fname)\n",
    "    # if os.path.exists(output_loc):\n",
    "    #     continue\n",
    "    if chunk_fname in [\".ipynb_checkpoints\"]:\n",
    "        continue\n",
    "    transcripts_df_chunk = pd.read_csv(os.path.join(transcripts_df_patch_dir, chunk_fname))\n",
    "    bin_df_chunk = generate_synthetic_VesiumHD_data(transcripts_df_chunk, bin_size, whole_cell=True, QScore20=True)\n",
    "    bin_df_chunk['column'] = bin_df_chunk['column']*2-1\n",
    "    bin_df_chunk['row'] = bin_df_chunk['row']*2-1\n",
    "    bin_df_chunk['geometry'] = generate_bin_polys(bin_df_chunk, 'column', 'row', 2)\n",
    "    bin_gdf_chunk = gpd.GeoDataFrame( bin_df_chunk, geometry = bin_df_chunk['geometry'])\n",
    "    bin_df_chunk.to_csv(output_loc)\n",
    "    print(f\"Successfully assigned transcripts to bins for {chunk_fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e310d-2a9d-41b5-9450-23ab3e57e7f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate cell_gdf as enact_pipeline input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d33fd-45be-4dde-b4b9-acc3de13f9e0",
   "metadata": {},
   "source": [
    "This session generate the cell_df patches required to run the enact pipeline. The main purpose is to create Shapely polygons that represent the cell outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4bff77-1b7a-4921-a4c2-0b66cf800468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polygons(coords_array):\n",
    "    polygons = []\n",
    "    for row in coords_array:\n",
    "        reshaped_coords = row.reshape(-1, 2)\n",
    "        polygon = Polygon(reshaped_coords)\n",
    "        polygons.append(polygon)\n",
    "    return polygons\n",
    "\n",
    "# Create the polygons\n",
    "polygons = create_polygons(file)\n",
    "cells_df['polygons'] = polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22875d42-5489-4ed0-b370-d693f26318e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_gdf_chunk = gpd.GeoDataFrame(cells_df, geometry = cells_df['polygons'])\n",
    "cell_gdf_chunk.rename(columns={'x_centroid': 'cell_x', 'y_centroid': 'cell_y'}, inplace=True)\n",
    "cell_gdf_chunk.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "cell_gdf_chunk[['cell_id','cell_x','cell_y','geometry']].to_csv(os.path.join(enact_data_dir, \"cells_gdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38a13d-3dfa-45e2-abc3-1b40c382a1db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run ENACT bin-to-cell pipeline\n",
    "In the configs.yaml file: \n",
    "\n",
    "    Set \"analysis_name\" in the configs.yaml file to \"xenium\" (or \"xenium_nuclei).\n",
    "    Set \"run_synthetic\" to True and all other steps to False.\n",
    "    Set \"bin_to_cell_method\" to one of these four: \"naive\", \"weighted_by_area\", \"weighted_by_gene\", or \"weighted_by_cluster\"\n",
    "\n",
    "Run `make run_enact`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8aa8e-0a17-48ae-86ed-81a04ec203dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670974eb-8dae-4d67-b735-1cd53858d560",
   "metadata": {},
   "source": [
    "The following cell will generate and save the ground truth of the synthetic VisiumHD data for the use of bin-to-cell assignment methods evaluation. Ground truth dataframe consists of rows representing the transcript counts of each cell. Each column represents a gene feature (gene feature name is also the column name)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224ea02-5701-450c-9efb-c38de7492764",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate Cell-gene matrix for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23be59-86ef-4ed0-b9fd-b22b203fa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ground_truth_table(transcripts_df, cells_df, whole_cell=True, QScore20=True, include_unassigned_transcript=False):\n",
    "    filtered_df = transcripts_df\n",
    "    \n",
    "    # only count transcripts in the nucleus\n",
    "    if not whole_cell:\n",
    "        filtered_df = transcripts_df[transcripts_df['overlaps_nucleus'] == 1]\n",
    "    \n",
    "    # only count transcripts with QScore >= 20\n",
    "    if QScore20:\n",
    "        filtered_df = filtered_df[filtered_df['qv'] >= 20]\n",
    "    \n",
    "    # only count transcripts that are assigned to specific cells\n",
    "    if not include_unassigned_transcript:\n",
    "        filtered_df = filtered_df[filtered_df['cell_id'] != 'UNASSIGNED']\n",
    "    \n",
    "    pivot_df = filtered_df.pivot_table(index='cell_id', columns='feature_name', aggfunc='size', fill_value=0)\n",
    "    \n",
    "    merged_df = pivot_df.merge(cells_df[['cell_id']], left_index=True, right_on='cell_id', how='right')\n",
    "    columns = ['cell_id'] + [col for col in merged_df.columns if col not in ['cell_id', 'x_centroid', 'y_centroid','polygons']]\n",
    "    merged_df = merged_df[columns]\n",
    "    merged_df.set_index('cell_id', inplace=True)\n",
    "    #merged_df['total_gene_counts'] = merged_df.iloc[:, 3:].sum(axis=1)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f2644-5496-4286-961c-fa74ea32e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 2\n",
    "cell_df_chunks = os.listdir(cells_df_chunks_dir)\n",
    "for chunk_fname in cell_df_chunks:\n",
    "    output_loc = os.path.join(ground_truth_dir,chunk_fname)\n",
    "    if os.path.exists(output_loc):\n",
    "        continue\n",
    "    if chunk_fname in [\".ipynb_checkpoints\"]:\n",
    "        continue\n",
    "    cell_df_chunk = pd.read_csv(os.path.join(cell_dir, chunk_fname))\n",
    "    groundtruth_chunk = generate_ground_truth_table(transcripts_df, cell_df_chunk, whole_cell=False, QScore20=False, include_unassigned_transcript=False)\n",
    "    groundtruth_chunk.to_csv(output_loc)\n",
    "    print(f\"Successfully generated groundthuth for {chunk_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a648b8-c2d9-4489-951e-dc0c443b489d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation of ENACT bin-to-cell results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759f86f-8498-41b1-a7ea-ca934b102d22",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Overall precision, recall, and f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f300f2-73fb-4c86-9bf0-704f053d5299",
   "metadata": {},
   "source": [
    "Run this session with all the methods you have run with ENACT, change 'method' in the cell bellow to the one you want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061ee46-1591-4a96-8643-5e96d7c55a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "method = \"weighted_by_cluster\"\n",
    "results_dir = os.path.join(enact_data_dir, method, \"bin_to_cell_assign\")\n",
    "\n",
    "# Initialize variables to accumulate weighted precision, recall, and F1\n",
    "total_cells = 0\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "missing_cells_count = 0\n",
    "total_cells_count = 0\n",
    "results_chunks = os.listdir(results_dir)\n",
    "\n",
    "for chunk_fname in results_chunks:\n",
    "    if chunk_fname in [\".ipynb_checkpoints\"]:\n",
    "        continue\n",
    "\n",
    "    generated = pd.read_csv(os.path.join(results_dir, chunk_fname))\n",
    "    ground_truth = pd.read_csv(os.path.join(ground_truth_dir, chunk_fname))\n",
    "    if len(generated) ==0:\n",
    "        print(chunk_fname)\n",
    "        continue\n",
    "    generated.rename(columns={'id': 'cell_id'}, inplace=True)\n",
    "  \n",
    "    # Align both dataframes by 'cell_id', filling missing cells in generated with 0\n",
    "    merged = pd.merge(ground_truth, generated, on='cell_id', how='left', suffixes=('_gt', '_gen')).fillna(0)\n",
    "    num_cells = (ground_truth.iloc[:, 1:] != 0).any(axis=1).sum()\n",
    "    missing_cells_count += num_cells - len(generated)\n",
    "    total_cells_count += num_cells\n",
    "\n",
    "    ground_truth_aligned = merged.filter(like='_gt').values\n",
    "    generated_aligned = merged.filter(like='_gen').values\n",
    "    assert ground_truth_aligned.shape == generated_aligned.shape, \"Aligned matrices must have the same shape!\"\n",
    "\n",
    "    num_cells = ground_truth_aligned.shape[0]\n",
    "\n",
    "    # Compute precision for the current patch\n",
    "    patch_precision = np.sum(np.minimum(generated_aligned, ground_truth_aligned)) / np.sum(generated_aligned)\n",
    "\n",
    "    # Compute recall for the current patch\n",
    "    patch_recall = np.sum(np.minimum(generated_aligned, ground_truth_aligned)) / np.sum(ground_truth_aligned)\n",
    "\n",
    "    # F1 score for the current patch\n",
    "    if patch_precision + patch_recall > 0:\n",
    "        patch_f1 = 2 * (patch_precision * patch_recall) / (patch_precision + patch_recall)\n",
    "    else:\n",
    "        patch_f1 = 0\n",
    "\n",
    "    # Accumulate the weighted precision, recall, and number of aligned cells\n",
    "    precision_sum += patch_precision * num_cells\n",
    "    recall_sum += patch_recall * num_cells\n",
    "    total_cells += num_cells\n",
    "    \n",
    "#  Compute overall weighted precision, recall, and F1 score\n",
    "overall_precision = precision_sum / total_cells\n",
    "overall_recall = recall_sum / total_cells\n",
    "\n",
    "if overall_precision + overall_recall > 0:\n",
    "    overall_f1_score = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall)\n",
    "else:\n",
    "    overall_f1_score = 0 \n",
    "\n",
    "# Print results\n",
    "print(f\"Overall Precision: {overall_precision}\")\n",
    "print(f\"Overall Recall: {overall_recall}\")\n",
    "print(f\"Overall F1 Score: {overall_f1_score}\")\n",
    "print(f\"Total missing cells in the generated data compared to ground truth: {missing_cells_count}\")\n",
    "print(f\"Total cells : {total_cells_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef397d4-ce75-4459-869e-7141fb72ba79",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualize the distribution using violin plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b763a9-4dce-48c3-9e43-40d2fbfd7c88",
   "metadata": {},
   "source": [
    "The following cells would create violin plots for all four methods in order to better compare the results. You can choose to only compare the ones you have run by changing the 'methods' list below to only include those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2326d-d85e-4075-afa5-2edf492eef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define methods and their directories\n",
    "methods = [\n",
    "    {\n",
    "        'name': 'Naive',\n",
    "        'results_dir': os.path.join(enact_data_dir, \"naive\", \"bin_to_cell_assign\"),       \n",
    "        'ground_truth_dir':ground_truth_dir\n",
    "    },\n",
    "    {\n",
    "        'name': 'Weighted_by_area',\n",
    "        'results_dir': os.path.join(enact_data_dir, \"weighted_by_area\", \"bin_to_cell_assign\"),       \n",
    "        'ground_truth_dir':ground_truth_dir\n",
    "    },\n",
    "    {\n",
    "        'name': 'Weighted_by_gene',\n",
    "        'results_dir':  os.path.join(enact_data_dir, \"weighted_by_gene\", \"bin_to_cell_assign\"),      \n",
    "        'ground_truth_dir': ground_truth_dir\n",
    "    },\n",
    "    {\n",
    "        'name': 'Weighted_by_cluster',\n",
    "        'results_dir':  os.path.join(enact_data_dir, \"weighted_by_cluster\", \"bin_to_cell_assign\"),   \n",
    "        'ground_truth_dir': ground_truth_dir\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize a list to store per-patch metrics for all methods\n",
    "metrics_list = []\n",
    "\n",
    "# Loop through each method to compute per-patch metrics\n",
    "for method in methods:\n",
    "    method_name = method['name']\n",
    "    results_dir = method['results_dir']\n",
    "    ground_truth_dir = method['ground_truth_dir']\n",
    "    \n",
    "    print(f\"Processing {method_name}...\")\n",
    "    \n",
    "    # Get list of generated and ground truth files\n",
    "    generated_files = [f for f in os.listdir(results_dir) if f.endswith('.csv') and f not in [\".ipynb_checkpoints\"]]\n",
    "    ground_truth_files = [f for f in os.listdir(ground_truth_dir) if f.endswith('.csv') and f not in [\".ipynb_checkpoints\"]]\n",
    "    \n",
    "    # Find common files between generated results and ground truth\n",
    "    common_files = set(generated_files) & set(ground_truth_files)\n",
    "    \n",
    "    if not common_files:\n",
    "        print(f\"No common files found for {method_name}. Skipping method.\")\n",
    "        continue\n",
    "    \n",
    "    # Loop through each common file (patch)\n",
    "    for fname in common_files:\n",
    "        ground_truth_path = os.path.join(ground_truth_dir, fname)\n",
    "        generated_path = os.path.join(results_dir, fname)\n",
    "        \n",
    "        # Load ground truth and generated data\n",
    "        ground_truth = pd.read_csv(ground_truth_path)\n",
    "        generated = pd.read_csv(generated_path)\n",
    "        \n",
    "        # Skip if generated data is empty\n",
    "        if generated.empty:\n",
    "            print(f\"No data in generated file {fname} for {method_name}. Skipping patch.\")\n",
    "            continue\n",
    "        \n",
    "        # Rename columns for consistency\n",
    "        if 'id' in generated.columns:\n",
    "            generated.rename(columns={'id': 'cell_id'}, inplace=True)\n",
    "        \n",
    "        # Merge ground truth and generated data on 'cell_id', filling missing values with 0\n",
    "        merged = pd.merge(\n",
    "            ground_truth, generated, on='cell_id', how='outer', suffixes=('_gt', '_gen')\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Extract aligned matrices for ground truth and generated data\n",
    "        ground_truth_aligned = merged.filter(regex='_gt$').values\n",
    "        generated_aligned = merged.filter(regex='_gen$').values\n",
    "        \n",
    "        # Ensure matrices are aligned\n",
    "        if ground_truth_aligned.shape != generated_aligned.shape:\n",
    "            print(f\"Shape mismatch in patch {fname} for {method_name}. Skipping patch.\")\n",
    "            continue\n",
    "        \n",
    "        # Compute counts for this patch\n",
    "        tp = np.sum(np.minimum(generated_aligned, ground_truth_aligned))\n",
    "        predicted = np.sum(generated_aligned)\n",
    "        actual = np.sum(ground_truth_aligned)\n",
    "        \n",
    "        # Compute metrics for this patch\n",
    "        precision = tp / predicted if predicted > 0 else 0\n",
    "        recall = tp / actual if actual > 0 else 0\n",
    "        f1_score = (\n",
    "            2 * (precision * recall) / (precision + recall)\n",
    "            if (precision + recall) > 0 else 0\n",
    "        )\n",
    "        \n",
    "        # Store metrics for this patch\n",
    "        metrics_list.append({\n",
    "            'Method': method_name,\n",
    "            'Patch': fname,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1_score\n",
    "        })\n",
    "\n",
    "# Create a DataFrame with per-patch metrics\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\nPer-Patch Metrics:\")\n",
    "print(metrics_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2f5b2-4b89-4480-85b5-6af2cc6bfb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure with subplots for each metric\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Precision Violin Plot\n",
    "sns.violinplot(x='Method', y='Precision', data=metrics_df, ax=axes[0], inner='quartile', palette='Set2')\n",
    "axes[0].set_title('Precision')\n",
    "axes[0].set_xlabel('Method')\n",
    "axes[0].set_ylabel('value')\n",
    "axes[0].set_ylim(0,1)\n",
    "axes[0].tick_params(axis='x', labelsize=8)  # Adjust the font size here\n",
    "\n",
    "# Recall Violin Plot\n",
    "sns.violinplot(x='Method', y='Recall', data=metrics_df, ax=axes[1], inner='quartile', palette='Set2')\n",
    "axes[1].set_title('Recall')\n",
    "axes[1].set_xlabel('Method')\n",
    "axes[1].set_ylabel('value')\n",
    "axes[1].set_ylim(0,1)\n",
    "axes[1].tick_params(axis='x', labelsize=8)  # Adjust the font size here\n",
    "\n",
    "# F1 Score Violin Plot\n",
    "sns.violinplot(x='Method', y='F1 Score', data=metrics_df, ax=axes[2], inner='quartile', palette='Set2')\n",
    "axes[2].set_title('F1 Score')\n",
    "axes[2].set_xlabel('Method')\n",
    "axes[2].set_ylabel('value')\n",
    "axes[2].set_ylim(0,1)\n",
    "axes[2].tick_params(axis='x', labelsize=8)  # Adjust the font size here\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
